{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Train BLIP on Wallet Captions in Google Colab\n",
                "\n",
                "**Instructions:**\n",
                "1. Make sure you are using a GPU runtime (**Runtime > Change runtime type > T4 GPU**).\n",
                "2. Upload `dataset.zip` to the Colab files pane on the left.\n",
                "3. Run the cells below!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!unzip -q -o dataset.zip\n",
                "print(\"‚úÖ Dataset extracted successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers torch torchvision Pillow matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import random\n",
                "from PIL import Image\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "print(\"--- üõ†Ô∏è PATH VALIDATION TEST ---\")\n",
                "try:\n",
                "    with open(\"wallet_captions.json\", \"r\") as f:\n",
                "        data = json.load(f)\n",
                "    keys = list(data.keys())\n",
                "    samples = random.sample(keys, min(3, len(keys)))\n",
                "    for key in samples:\n",
                "        pure_filename = key.replace('\\\\', '/').split('/')[-1]\n",
                "        colab_path = f\"/content/wallet/{pure_filename}\"\n",
                "        exists = os.path.exists(colab_path)\n",
                "        print(f\"\\nJSON Key: {key}\")\n",
                "        print(f\"Resolved Path: {colab_path}\")\n",
                "        print(f\"File Exists? {'‚úÖ YES' if exists else '‚ùå NO'}\")\n",
                "        if exists:\n",
                "            img = Image.open(colab_path)\n",
                "            plt.figure(figsize=(2, 2))\n",
                "            plt.imshow(img)\n",
                "            plt.title(f\"Found: {pure_filename}\")\n",
                "            plt.axis('off')\n",
                "            plt.show()\n",
                "        else:\n",
                "            print(f\"üö® ERROR: Cannot find image at {colab_path}. Please check your unzipped folder name!\")\n",
                "except Exception as e:\n",
                "    print(f\"Error during validation: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import time\n",
                "import torch\n",
                "from PIL import Image\n",
                "import matplotlib.pyplot as plt\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
                "from torch.optim import AdamW\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# --- CONFIGURATION ---\n",
                "JSON_FILE = \"wallet_captions.json\"\n",
                "MODEL_ID = \"Salesforce/blip-image-captioning-base\"\n",
                "EPOCHS = 3\n",
                "BATCH_SIZE = 4\n",
                "LEARNING_RATE = 5e-5\n",
                "SAVE_PATH = \"./finetuned_wallet_blip\"\n",
                "\n",
                "# --- DATA PREPARATION ---\n",
                "class WalletDataset(Dataset):\n",
                "    def __init__(self, json_path, processor):\n",
                "        print(f\"[INFO] Loading JSON annotations from {json_path}...\")\n",
                "        if not os.path.exists(json_path):\n",
                "            raise FileNotFoundError(f\"\\n‚ùå CRITICAL ERROR: Could not find {json_path}. Please make sure you uploaded and extracted dataset.zip!\")\n",
                "            \n",
                "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
                "            self.data_dict = json.load(f)\n",
                "            \n",
                "        self.image_paths = list(self.data_dict.keys())\n",
                "        self.processor = processor\n",
                "        print(f\"[INFO] Successfully loaded {len(self.image_paths)} annotations.\")\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self.image_paths)\n",
                "    \n",
                "    def _create_caption(self, features):\n",
                "        color = features.get(\"color\", \"unknown\").lower()\n",
                "        material = features.get(\"material_type\", \"unknown\").lower()\n",
                "        wallet_type = features.get(\"type_of_wallet\", \"wallet\").lower()\n",
                "        pattern = features.get(\"pattern\", \"solid\").lower()\n",
                "        brand = features.get(\"brand\", \"unknown\").lower()\n",
                "        \n",
                "        caption = f\"a {pattern} {color} {material} {wallet_type}\"\n",
                "        if brand != \"unknown\" and brand != \"\":\n",
                "            caption += f\" by {brand}\"\n",
                "        return caption\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        img_path_key = self.image_paths[idx]\n",
                "        features = self.data_dict[img_path_key]\n",
                "        \n",
                "        # STRIP WINDOWS PATHS: turns \"wallet\\\\image.jpg\" into \"image.jpg\"\n",
                "        pure_filename = img_path_key.replace('\\\\', '/').split('/')[-1]\n",
                "        \n",
                "        colab_path = f\"/content/wallet/{pure_filename}\"\n",
                "        \n",
                "        try:\n",
                "            image = Image.open(colab_path).convert(\"RGB\")\n",
                "        except Exception as e:\n",
                "            raise FileNotFoundError(f\"Missing image: {colab_path}. Checked key: {img_path_key}\")\n",
                "\n",
                "        caption = self._create_caption(features)\n",
                "        encoding = self.processor(images=image, text=caption, padding=\"max_length\", return_tensors=\"pt\")\n",
                "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
                "        return encoding\n",
                "\n",
                "# Load processor and Base Model\n",
                "print(f\"[INFO] Loading processor and model: {MODEL_ID}\")\n",
                "print(\"[INFO] This might take a minute as it downloads the base weights...\")\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"[INFO] Using compute device: {device.type.upper()}\")\n",
                "if device.type != \"cuda\":\n",
                "    print(\"‚ö†Ô∏è WARNING: You are not using a GPU! Training will be extremely slow. Please change runtime to T4 GPU.\")\n",
                "\n",
                "processor = BlipProcessor.from_pretrained(MODEL_ID)\n",
                "model = BlipForConditionalGeneration.from_pretrained(MODEL_ID)\n",
                "model.to(device)\n",
                "print(\"[INFO] Model loaded successfully to GPU.\")\n",
                "\n",
                "# Prepare Dataset & DataLoader\n",
                "dataset = WalletDataset(JSON_FILE, processor)\n",
                "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "print(f\"[INFO] DataLoader ready. Batches per epoch: {len(dataloader)}\")\n",
                "\n",
                "# Optimizer\n",
                "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"üöÄ Starting Training Loop...\")\n",
                "print(\"=\"*50 + \"\\n\")\n",
                "model.train()\n",
                "\n",
                "epoch_losses = []\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    start_time = time.time()\n",
                "    total_loss = 0\n",
                "    loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=True)\n",
                "    \n",
                "    for batch_idx, batch in enumerate(loop):\n",
                "        input_ids = batch[\"input_ids\"].to(device)\n",
                "        attention_mask = batch[\"attention_mask\"].to(device)\n",
                "        pixel_values = batch[\"pixel_values\"].to(device)\n",
                "        labels = input_ids.clone()\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(\n",
                "            input_ids=input_ids,\n",
                "            attention_mask=attention_mask,\n",
                "            pixel_values=pixel_values,\n",
                "            labels=labels\n",
                "        )\n",
                "        \n",
                "        loss = outputs.loss\n",
                "        total_loss += loss.item()\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        # Detailed progress tracking\n",
                "        loop.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n",
                "        \n",
                "    avg_loss = total_loss / len(dataloader)\n",
                "    epoch_losses.append(avg_loss)\n",
                "    elapsed_time = time.time() - start_time\n",
                "    print(f\"‚úÖ Epoch {epoch+1} Complete | Avg Loss: {avg_loss:.4f} | Time: {elapsed_time:.1f}s\")\n",
                "\n",
                "print(\"\\nüéâ Training finished!\")\n",
                "\n",
                "# Plotting the training loss\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(range(1, EPOCHS + 1), epoch_losses, marker='o', linestyle='-', color='b')\n",
                "plt.title('Training Loss over Epochs')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Average Loss')\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüíæ Saving the fine-tuned model to {SAVE_PATH}...\")\n",
                "os.makedirs(SAVE_PATH, exist_ok=True)\n",
                "model.save_pretrained(SAVE_PATH)\n",
                "processor.save_pretrained(SAVE_PATH)\n",
                "print(\"üíæ Model saved successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "print(\"\\n--- üîç RUNNING INFERENCE SANITY CHECK ---\")\n",
                "model.eval()\n",
                "\n",
                "# Pick a random image from the dataset\n",
                "sample_idx = random.randint(0, len(dataset) - 1)\n",
                "sample_img_path_key = dataset.image_paths[sample_idx]\n",
                "\n",
                "# STRIP WINDOWS PATHS: turns \"wallet\\\\image.jpg\" into \"image.jpg\"\n",
                "pure_filename = sample_img_path_key.replace('\\\\', '/').split('/')[-1]\n",
                "colab_path = f\"/content/wallet/{pure_filename}\"\n",
                "\n",
                "try:\n",
                "    test_img = Image.open(colab_path).convert(\"RGB\")\n",
                "    \n",
                "    # Show the image\n",
                "    plt.imshow(test_img)\n",
                "    plt.axis('off')\n",
                "    plt.title(f\"Testing: {pure_filename}\")\n",
                "    plt.show()\n",
                "    \n",
                "    inputs = processor(test_img, return_tensors=\"pt\").to(device)\n",
                "    # Generate prediction\n",
                "    out = model.generate(**inputs, max_new_tokens=50)\n",
                "    predicted_caption = processor.decode(out[0], skip_special_tokens=True)\n",
                "    \n",
                "    # Get ground truth\n",
                "    features = dataset.data_dict[sample_img_path_key]\n",
                "    ground_truth = dataset._create_caption(features)\n",
                "    \n",
                "    print(f\"üéØ Ground Truth JSON : {features}\")\n",
                "    print(f\"üìù Target Caption    : {ground_truth}\")\n",
                "    print(f\"ü§ñ Model Prediction  : {predicted_caption}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Inference failed: {e}\")\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
